# Notes et Am√©liorations par Fichier

## main.js (1240 lignes)

**En-t√™te ajout√© ‚úÖ**

### Am√©liorations Identifi√©es:

1. **Fichier trop volumineux** - 1240 lignes, devrait √™tre < 400
2. **Handlers IPC** - Extraire dans module s√©par√© (120+ handlers)
3. **Gestion config** - √âviter `delete require.cache` (ligne ~318)
4. **System tray** - Extraire dans module s√©par√©
5. **Gestion fen√™tre** - Extraire dans module s√©par√©

### Code Probl√©matique:

```javascript
// Ligne ~318 - Mauvaise pratique
delete require.cache[require.resolve("./sharkoder.config.json")];
const userConfig = require("./sharkoder.config.json");
// Solution: Utiliser ConfigManager.reload()
```

---

## preload.js (120 lignes)

**En-t√™te ajout√© ‚úÖ**

### Qualit√©: Excellent

- Code propre et bien organis√©
- Bonne utilisation de contextBridge
- S√©curit√© correcte
- **Aucune am√©lioration critique n√©cessaire**

---

## backend/config.js (221 lignes)

**En-t√™te ajout√© ‚úÖ**

### Qualit√©: Tr√®s bon

- Pattern singleton bien impl√©ment√©
- Watchers pour changements

### Am√©liorations Sugg√©r√©es:

1. Ajouter validation de configuration au chargement
2. Syst√®me de migrations pour changements de structure
3. M√©canisme de rollback en cas d'erreur

---

## backend/db.js (491 lignes)

**En-t√™te ajout√© ‚úÖ**  
**Note sp√©ciale dans en-t√™te:** "AM√âLIORATION RECOMMAND√âE: Refactoriser migrations"

### PROBL√àME MAJEUR:

**Lignes 56-133** - 13 blocs try/catch identiques pour migrations

```javascript
try {
  db.run("ALTER TABLE jobs ADD COLUMN container TEXT");
} catch (e) {
  /* Column already exists */
}
// ... r√©p√©t√© 13 fois
```

### Solution D√©taill√©e:

```javascript
// Cr√©er table de versions
db.run(`CREATE TABLE IF NOT EXISTS schema_version (
  version INTEGER PRIMARY KEY,
  applied_at TEXT DEFAULT CURRENT_TIMESTAMP
)`);

// Liste de migrations
const migrations = [
  { version: 1, sql: "ALTER TABLE jobs ADD COLUMN container TEXT" },
  { version: 2, sql: "ALTER TABLE jobs ADD COLUMN resolution TEXT" },
  // ... etc
];

// Fonction d'application
async function runMigrations() {
  const currentVersion = getCurrentSchemaVersion();
  for (const migration of migrations) {
    if (migration.version > currentVersion) {
      db.run(migration.sql);
      db.run("INSERT INTO schema_version (version) VALUES (?)", [migration.version]);
    }
  }
}
```

### Impact:

- ‚úÖ R√©duction de ~80 lignes
- ‚úÖ Migrations tra√ßables
- ‚úÖ Plus maintenable

---

## backend/queue.js (1097 lignes)

**En-t√™te ajout√© ‚úÖ**  
**Note sp√©ciale:** "AM√âLIORATIONS RECOMMAND√âES: Extraire en sous-modules"

### PROBL√àME: Fichier trop complexe

Responsabilit√©s multiples:

- Gestion file d'attente
- Pipeline t√©l√©chargement
- Pipeline encodage
- Pipeline upload
- Gestion backups
- Retry logic
- Cleanup

### D√©composition Recommand√©e:

```
queue.js (300 lignes) - Orchestrateur principal
‚îú‚îÄ‚îÄ download-handler.js (150 lignes)
‚îú‚îÄ‚îÄ encode-handler.js (200 lignes)
‚îú‚îÄ‚îÄ upload-handler.js (150 lignes)
‚îú‚îÄ‚îÄ backup-manager.js (100 lignes)
‚îî‚îÄ‚îÄ retry-logic.js (100 lignes)
```

### Machine √† √âtats Sugg√©r√©e:

```javascript
const JobStates = {
  WAITING: "waiting",
  DOWNLOADING: "downloading",
  READY_ENCODE: "ready_encode",
  ENCODING: "encoding",
  READY_UPLOAD: "ready_upload",
  UPLOADING: "uploading",
  AWAITING_APPROVAL: "awaiting_approval",
  COMPLETED: "completed",
  FAILED: "failed",
  PAUSED: "paused",
};

const StateTransitions = {
  [JobStates.WAITING]: [JobStates.DOWNLOADING, JobStates.PAUSED],
  [JobStates.DOWNLOADING]: [JobStates.READY_ENCODE, JobStates.FAILED],
  // ... etc
};
```

---

## backend/encode.js (617 lignes)

**En-t√™te ajout√© ‚úÖ**  
**Note sp√©ciale:** "AM√âLIORATIONS RECOMMAND√âES: Support GPU AMD/Intel"

### Qualit√©: Bon

### Am√©liorations Sugg√©r√©es:

1. **Support GPU autres que NVIDIA**

   ```javascript
   // Ajouter d√©tection AMD VCE
   if (hasAMD()) return "hevc_amf";
   // Ajouter d√©tection Intel QSV
   if (hasIntel()) return "hevc_qsv";
   ```

2. **Cache test GPU**

   ```javascript
   // Sauvegarder r√©sultat du test
   const GPU_CACHE_FILE = ".gpu_capabilities.json";
   ```

3. **Optimiser logs d√©tection r√©solution**
   - Lignes 67-69 - Logs r√©p√©titifs
   - Logger une seule fois avec niveau DEBUG

### Point Fort:

‚úÖ Test GPU avec fallback CPU bien impl√©ment√©

---

## backend/transfer.js (369 lignes)

**En-t√™te ajout√© ‚úÖ**  
**Note sp√©ciale:** "AM√âLIORATIONS RECOMMAND√âES: M√©triques, cache capacit√©s"

### Qualit√©: Excellent

### Architecture Intelligente:

- ‚úÖ S√©lection auto du meilleur protocole
- ‚úÖ Fallback automatique
- ‚úÖ D√©tection WebDAV read-only (403)

### Am√©liorations Sugg√©r√©es:

1. **M√©triques de performance**

   ```javascript
   class TransferMetrics {
     trackUpload(method, size, duration) {
       // Calculer moyenne par protocole
     }

     getBestMethod(operation) {
       // Choisir bas√© sur historique
     }
   }
   ```

2. **Cache capacit√©s serveur**
   ```javascript
   const serverCapabilities = {
     webdav: { canUpload: false, canDelete: true },
     sftp: { canUpload: true, canDelete: true },
   };
   ```

---

## backend/utils.js (313 lignes)

**En-t√™te ajout√© ‚úÖ**  
**Note sp√©ciale:** "AM√âLIORATIONS RECOMMAND√âES: Rotation logs, espace disque r√©el"

### Qualit√©: Bon

### Probl√®mes Identifi√©s:

#### 1. Pas de rotation des logs

```javascript
// Actuel: append infini
fs.appendFile(this.logFile, logMessage + "\n");

// Sugg√©r√©: rotation avec winston
const winston = require("winston");
require("winston-daily-rotate-file");

const logger = winston.createLogger({
  transports: [
    new winston.transports.DailyRotateFile({
      filename: "sharkoder-%DATE%.log",
      datePattern: "YYYY-MM-DD",
      maxSize: "20m",
      maxFiles: "14d",
    }),
  ],
});
```

#### 2. Placeholder espace disque (lignes 67-77)

```javascript
// Actuel: valeurs hardcod√©es
return {
  free: 50 * 1024 * 1024 * 1024, // Placeholder!
  size: 100 * 1024 * 1024 * 1024,
};

// Sugg√©r√©: vraie d√©tection
const checkDiskSpace = require("check-disk-space");
const diskSpace = await checkDiskSpace(dirPath);
return diskSpace;
```

#### 3. Hash MD5 uniquement

```javascript
// Ajouter SHA256
const calculateFileHashSHA256 = async (filePath) => {
  const hash = crypto.createHash("sha256");
  const stream = fs.createReadStream(filePath);
  // ...
};
```

---

## backend/sftp.js (785 lignes)

**En-t√™te ajout√© ‚úÖ**  
**Note sp√©ciale:** "AM√âLIORATIONS: Extraire getBackupPath, BaseTransferManager"

### Qualit√©: Bon

### Points Forts:

‚úÖ Optimisations SSH excellentes:

- AES-GCM cipher (le plus rapide avec acc√©l√©ration hardware)
- Buffers 256KB
- Keepalive configur√©
- Pas de compression (inutile pour vid√©os)

### CODE DUPLIQU√â (lignes 9-12):

```javascript
// DUPLIQU√â dans webdav.js aussi!
function getBackupPath(originalPath) {
  const parsedPath = path.posix.parse(originalPath);
  return path.posix.join(parsedPath.dir, `${parsedPath.name}.bak${parsedPath.ext}`);
}

// SOLUTION: Ajouter √† utils.js
```

### Am√©liorations:

1. **Classe abstraite BaseTransferManager**

   ```javascript
   class BaseTransferManager {
     async downloadFile(remotePath, localPath, onProgress) {
       // Logique commune
     }

     async uploadFile(localPath, remotePath, onProgress) {
       // Logique commune
     }
   }

   class SftpManager extends BaseTransferManager {
     // Impl√©mentation sp√©cifique
   }
   ```

2. **Connection pooling**
   ```javascript
   class SftpConnectionPool {
     constructor(maxConnections = 3) {
       this.pool = [];
     }

     async getConnection() {
       // R√©utiliser connexions
     }
   }
   ```

---

## backend/webdav.js (713 lignes)

**En-t√™te ajout√© ‚úÖ**  
**Note sp√©ciale:** "CODE DUPLIQU√â D√âTECT√â: getBackupPath, progress tracking, backup logic"

### Qualit√©: Bon

### CODE DUPLIQU√â (lignes 10-13):

```javascript
// IDENTIQUE √† sftp.js!
function getBackupPath(originalPath) {
  const parsedPath = path.posix.parse(originalPath);
  return path.posix.join(parsedPath.dir, `${parsedPath.name}.bak${parsedPath.ext}`);
}
```

### Progress Tracking Similaire:

Lignes 178-210 (download) et 349-371 (upload) tr√®s similaires √† sftp.js

**SOLUTION: Classe ProgressTracker**

```javascript
// Ajouter √† utils.js
class ProgressTracker {
  constructor(totalSize) {
    this.totalSize = totalSize;
    this.transferredSize = 0;
    this.startTime = Date.now();
    this.lastUpdate = this.startTime;
    this.lastSize = 0;
  }

  onData(chunk, onProgress) {
    this.transferredSize += chunk.length;
    const now = Date.now();

    if (now - this.lastUpdate >= 500) {
      // Update every 500ms
      const progress = (this.transferredSize / this.totalSize) * 100;
      const speed = (this.transferredSize - this.lastSize) / ((now - this.lastUpdate) / 1000);
      const eta = speed > 0 ? (this.totalSize - this.transferredSize) / speed : 0;

      onProgress({
        progress,
        transferred: this.transferredSize,
        total: this.totalSize,
        speed,
        eta,
        elapsedTime: (now - this.startTime) / 1000,
      });

      this.lastUpdate = now;
      this.lastSize = this.transferredSize;
    }
  }
}

// Usage
const tracker = new ProgressTracker(totalSize);
stream.on("data", (chunk) => tracker.onData(chunk, onProgress));
```

### Backup Logic Dupliqu√©e:

Lignes 271-291 identiques √† sftp.js lignes 325-345

**SOLUTION: BackupManager**

```javascript
// Cr√©er backend/backup-manager.js
class BackupManager {
  static async createBackup(client, remotePath, config) {
    const enabled = config.advanced?.create_backups !== false;

    if (!enabled) {
      logger.info(`Backup disabled for: ${remotePath}`);
      return null;
    }

    const backupPath = getBackupPath(remotePath);
    const exists = await client.exists(remotePath);

    if (exists) {
      await client.rename(remotePath, backupPath);
      logger.info(`Backup created: ${backupPath}`);
      return backupPath;
    }

    return null;
  }

  static async deleteBackup(client, backupPath) {
    // Supprimer backup apr√®s upload r√©ussi
  }

  static async restoreBackup(client, backupPath, originalPath) {
    // Restaurer en cas d'√©chec
  }
}
```

---

## backend/progressfile.js (339 lignes)

**En-t√™te ajout√© ‚úÖ**  
**Note sp√©ciale:** "AM√âLIORATIONS: Lock multi-instances, versioning format, compression"

### Qualit√©: Bon

### Point Fort:

‚úÖ Upload atomique bien impl√©ment√© (temp + rename)

### Am√©liorations Sugg√©r√©es:

#### 1. Lock fichier (multi-instances)

```javascript
class ProgressFileManager {
  async acquireLock() {
    const lockFile = `${this.progressFilePath}.lock`;
    const lockTimeout = 30000; // 30s

    const startTime = Date.now();
    while (await this.sftpManager.fileExists(lockFile)) {
      if (Date.now() - startTime > lockTimeout) {
        throw new Error("Lock timeout");
      }
      await sleep(1000);
    }

    await this.sftpManager.uploadFile(Buffer.from(process.pid.toString()), lockFile);
  }

  async releaseLock() {
    const lockFile = `${this.progressFilePath}.lock`;
    await this.sftpManager.deleteFile(lockFile);
  }
}
```

#### 2. Versioning format

```javascript
getDefaultProgressStructure() {
  return {
    meta: {
      version: "2.0",  // Format version
      schema_version: 2,
      // ...
    },
    jobs: []
  };
}

async loadProgressFile() {
  const data = await this.downloadProgressFile();

  // Migration si ancienne version
  if (data.meta.schema_version < 2) {
    data = this.migrateToV2(data);
  }

  return data;
}
```

#### 3. Compression/Rotation

```javascript
async saveProgressFile(progressData) {
  // Si > 10MB, archiver l'ancien
  if (progressData.jobs.length > 10000) {
    await this.archiveOldJobs(progressData);
  }

  // Compression optionnelle
  if (this.config.advanced?.compress_progress) {
    const compressed = zlib.gzipSync(JSON.stringify(progressData));
    await this.uploadCompressed(compressed);
  }
}
```

#### 4. Int√©grit√© (checksum)

```javascript
async uploadProgressFile(progressData) {
  const json = JSON.stringify(progressData);
  const checksum = crypto.createHash('sha256').update(json).digest('hex');

  progressData.meta.checksum = checksum;

  // Upload
  await this.uploadFile(progressData);
}

async loadProgressFile() {
  const data = await this.downloadProgressFile();
  const storedChecksum = data.meta.checksum;
  delete data.meta.checksum;

  const calculatedChecksum = crypto.createHash('sha256')
    .update(JSON.stringify(data))
    .digest('hex');

  if (storedChecksum !== calculatedChecksum) {
    throw new Error('Progress file corrupted (checksum mismatch)');
  }

  return data;
}
```

---

## R√©sum√© Global

### En-t√™tes Ajout√©s: ‚úÖ 11/11

### Code Dupliqu√© Identifi√©: 3 occurrences

1. **getBackupPath()** - sftp.js et webdav.js
2. **Progress tracking** - logique similaire dans les 2
3. **Backup logic** - pattern identique

### Fichiers √† Refactorer en Priorit√©:

1. **queue.js** (1097 lignes) ‚Üí D√©composer en 5 modules
2. **main.js** (1240 lignes) ‚Üí D√©composer en 4 modules
3. **db.js** (491 lignes) ‚Üí Refactorer migrations

### Am√©liorations par Urgence:

#### üî¥ HAUTE

- Syst√®me migrations DB versionn√©es
- D√©composition queue.js
- D√©composition main.js

#### üü° MOYENNE

- Factoriser code dupliqu√© (getBackupPath, ProgressTracker, BackupManager)
- Vraie d√©tection espace disque
- Rotation des logs

#### üü¢ BASSE

- Support GPU AMD/Intel
- Tests unitaires
- Documentation architecture

---

**Total am√©liorations identifi√©es:** 25+  
**Dette technique estim√©e:** ~15 jours de refactoring  
**Gain attendu:** -30% lignes, +50% maintenabilit√©, -40% bugs potentiels
